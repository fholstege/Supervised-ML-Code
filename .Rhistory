print(dim(mBeta))
print(dim(mX_test))
yhat_test = result$intercept  + cbind(1,mX_test) %*% mBeta
# calculate RMSE for this fold
error <- mY_test - yhat_test
fold_RMSE <- sqrt(1/nrow(mX_test)) * (t(error)%*%error)
# add to total RMSE to later average out
total_RMSE = total_RMSE + fold_RMSE
}
# compute average RMSE
avg_RMSE = total_RMSE/length(folds)
# define results, including parameters
result = list(avg_RMSE = avg_RMSE,
param = param)
return(result)
}
# apply grid search for all the types of kernels
grid_result_linear <- hyperparam_search(mX_scaled, mY, k, type_kernel = "linear", vlambda = vlambda)
grid_result_nonhompolynom <- hyperparam_search(mX_scaled, mY, k, type_kernel = "nonhompolynom", vlambda = vlambda, vdegree = vdegree)
grid_result_RBF <- hyperparam_search(mX_scaled, mY, k, type_kernel = "RBF", vlambda = vlambda, vgamma = vgamma)
# get optimal parameters for each kernel
optimal_param_linear <- grid_result_linear[which.min(grid_result_linear$avg_RSME),]
optimal_param_nonhompolynom <- grid_result_nonhompolynom[which.min(grid_result_nonhompolynom$avg_RSME),]
optimal_param_RBF <- grid_result_RBF[which.min(grid_result_RBF$avg_RSME),]
# train the data with optimal parameters
result_linear = kernel_ridge(mX_scaled, mY, type_kernel = "linear", param = param_t)
result_nonhompolynom = kernel_ridge(mX_scaled, mY, type_kernel = "nonhompolynom", param = optimal_param_nonhompolynom)
result_RBF = kernel_ridge(mX_scaled, mY, type_kernel = "RBF", param = optimal_param_RBF)
# train the data with optimal parameters
result_linear = kernel_ridge(mX_scaled, mY, type_kernel = "linear", param = optimal_param_linear)
result_RBF = kernel_ridge(mX_scaled, mY, type_kernel = "RBF", param = optimal_param_RBF)
result_nonhompolynom = kernel_ridge(mX_scaled, mY, type_kernel = "nonhompolynom", param = optimal_param_nonhompolynom)
## linear kernel
# for the linear kernel, we get exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_linear = krr(y=mY, X= mX, kernel.type = "linear", lambda = optimal_param_linear$lambda)
result_dsmle_linear$yhat - result_linear$yhat
## linear kernel
# for the linear kernel, we get exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_linear = krr(y=mY, X= mX, kernel.type = "linear", lambda = optimal_param_linear$lambda)
result_dsmle_linear$yhat - result_linear$yhat
# for the non homogeneous polynomial kernel, exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_nonhompolynom = krr(y=mY, X= mX, kernel.type = "nonhompolynom",
lambda = optimal_param_nonhompolynom$lambda,
kernel.degree = optimal_param_nonhompolynom$degree)
result_dsmle_nonhompolynom$yhat - (result_nonhompolynom$yhat)
create_kernel = function(type_kernel, mXXt, mX, param){
##create kernel based on type chosen
if(type_kernel == "linear"){
K = mXXt
}else if(type_kernel == "nonhompolynom"){
K = ((1 + mXXt)^param$degree)
}else if(type_kernel == "RBF"){
K = exp(-as.matrix(dist(mX)^2) * param$gamma)
}
return(K)
}
grid_result_nonhompolynom <- hyperparam_search(mX_scaled, mY, k, type_kernel = "nonhompolynom", vlambda = vlambda, vdegree = vdegree)
optimal_param_nonhompolynom <- grid_result_nonhompolynom[which.min(grid_result_nonhompolynom$avg_RSME),]
result_nonhompolynom = kernel_ridge(mX_scaled, mY, type_kernel = "nonhompolynom", param = optimal_param_nonhompolynom)
# for the non homogeneous polynomial kernel, exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_nonhompolynom = krr(y=mY, X= mX, kernel.type = "nonhompolynom",
lambda = optimal_param_nonhompolynom$lambda,
kernel.degree = optimal_param_nonhompolynom$degree)
result_dsmle_nonhompolynom$yhat - (result_nonhompolynom$yhat)
create_kernel = function(type_kernel, mXXt, mX, param){
##create kernel based on type chosen
if(type_kernel == "linear"){
K = mXXt
}else if(type_kernel == "nonhompolynom"){
K = ((1 + mXXt)^param$degree)-1
}else if(type_kernel == "RBF"){
K = exp(-as.matrix(dist(mX)^2) * param$gamma)
}
return(K)
}
# Goal: Implementation of kernel ridge regression, comparision with dsmle package, including k-fold cv
# Author: Floris Holstege, Ruoying Dai, Ruben Eschauzier, Joyce Zhi
#
# Put here the packages required for subsequent analysis. P_load ensures these will be installed and loaded.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(stargazer,
lmtest,
matlib, # install dependencies for dsmle
SVMMaj,
ISLR,
plotrix,
e1071,
ggplot2,
reshape2,
tidyverse)
# get local package, dsmle, and add
install.packages("dsmle_1.0-4.tar.gz", repos = NULL, type="source")
library(dsmle)
create_kernel = function(type_kernel, mXXt, mX, param){
##create kernel based on type chosen
if(type_kernel == "linear"){
K = mXXt
}else if(type_kernel == "nonhompolynom"){
K = ((1 + mXXt)^param$degree)-1
}else if(type_kernel == "RBF"){
K = exp(-as.matrix(dist(mX)^2) * param$gamma)
}
return(K)
}
# implements k-fold cross validation for kernel ridge regression
cv_kernel_ridge <- function(mX, mY, lambda, type_kernel,folds,param){
# initial value for total rmse, min and max
total_RMSE <- 0
#Perform k fold cross validation
for(i in 1:length(folds)){
#Split the data according to the folds
test = folds[[i]]
train = -folds[[i]]
# define train and test set for y and x
mY_train <- mY[train,]
mX_train <- mX[train,]
mY_test <- mY[test,]
mX_test <- mX[test,]
# get result from training set
result = kernel_ridge(mX_train,mY_train,type_kernel,param)
# get beta's and predict on the test set
# Likely something going wrong here - get different RMSE than dsmle
mBeta = rbind(result$intercept, result$Beta)
yhat_test = result$intercept  + cbind(1,mX_test) %*% mBeta
# calculate RMSE for this fold
error <- mY_test - yhat_test
fold_RMSE <- sqrt(1/nrow(mX_test)) * (t(error)%*%error)
# add to total RMSE to later average out
total_RMSE = total_RMSE + fold_RMSE
}
# compute average RMSE
avg_RMSE = total_RMSE/length(folds)
# define results, including parameters
result = list(avg_RMSE = avg_RMSE,
param = param)
return(result)
}
# executes kernel ridge regression
kernel_ridge <- function(mX, mY, type_kernel = "linear", param){
# define n, p
n = nrow(mX)
p = ncol(mX)
# initial XX^T, X^TX
mXXt = mX %*% t(mX)
mXtX = t(mX) %*% mX
# create kernel
K = create_kernel(type_kernel, mXXt, mX, param)
# define J, using vector of 1s
v_1 =  matrix(1, 1, n)
J = diag(n)- ((1/n)*  t(v_1) %*% v_1)[[1]]
# define U and d
D = diag(1/eigen(K)$values)
U = eigen(K)$vectors
# find optimal intercept
optimal_mBeta_zero = (1/n) * v_1 %*% mY
# define shrunk matrix first, then find optimal intercept
mShrunk = eigen(K)$values / (eigen(K)$values + outer(rep(1,n), param$lambda))
optimal_q_hat = U %*% (mShrunk * as.vector(t(U) %*% mY))
# from q's, get other beta's
beta_from_q_hat =  inv(mXtX) %*% t(mX) %*% optimal_q_hat
# predict Y-hat
yhat = optimal_q_hat + optimal_mBeta_zero[[1]]
## adaptation to ensure correct Y-hat with RBF kernel
if (type_kernel == "RBF"){
yhat = optimal_q_hat
}
# get error and RMSE
error = mY - yhat
RMSE = sqrt((1/n) * (t(error)%*%error))
# return results as list
result = list(Beta = beta_from_q_hat,
intercept = optimal_mBeta_zero[[1]],
qhat = optimal_q_hat,
yhat = yhat,
X = mX,
y = mY,
RMSE = RMSE,
K = K,
lambda = param$lambda,
param = param
)
return(result)
}
# search for best lambda and other parameters, using grid search
hyperparam_search = function(mX, mY, k,type_kernel, vlambda, vdegree = NA, vgamma = NA){
# define parameter grid based on type of kernel
if(type_kernel == "linear"){
paramGrid <- expand.grid(vlambda)
colnames(paramGrid) <- c("lambda")
}else if(type_kernel == "RBF"){
paramGrid <- expand.grid(vlambda, vgamma)
colnames(paramGrid) <- c("lambda", "gamma")
}else if(type_kernel == "nonhompolynom"){
paramGrid <- expand.grid(vlambda, vdegree)
colnames(paramGrid) <- c("lambda", "degree")
}
# create k equally size folds
folds = createFolds(mY, k = k, list = TRUE, returnTrain = FALSE)
# empty column, later to be filled
paramGrid$avg_RSME <- NA
# iterate over the grid
for(i in 1:nrow(paramGrid)){
# select parameters from the grid
param <- paramGrid[i,]
# test these with k-fold
cv_result <- cv_kernel_ridge(mX, mY, type_kernel = type_kernel, folds = folds, param = param)
#save the result
paramGrid$avg_RSME[i] <- cv_result$avg_RMSE
}
return(paramGrid)
}
### section 1: data preparation
# get dataset
setwd("KRR")
load("Airline.Rdata")
# set seed for reproduction
set.seed(123)
# define independent and dependent variables
mX = as.matrix(Airline[,-4])
mX_scaled = scale(mX)
mY = as.matrix(Airline[,4])
# define the parameters to try out in the gridsearch
vlambda = 10^seq(-5, 5, length.out = 50)
vdegree = seq(1,5,1)
vgamma = ncol(mX_scaled)^seq(-2,3,length.out =5)
k = 5
### section 2: finding the optimal parameters
# apply grid search for all the types of kernels
grid_result_linear <- hyperparam_search(mX_scaled, mY, k, type_kernel = "linear", vlambda = vlambda)
grid_result_nonhompolynom <- hyperparam_search(mX_scaled, mY, k, type_kernel = "nonhompolynom", vlambda = vlambda, vdegree = vdegree)
grid_result_RBF <- hyperparam_search(mX_scaled, mY, k, type_kernel = "RBF", vlambda = vlambda, vgamma = vgamma)
# get optimal parameters for each kernel
optimal_param_linear <- grid_result_linear[which.min(grid_result_linear$avg_RSME),]
optimal_param_nonhompolynom <- grid_result_nonhompolynom[which.min(grid_result_nonhompolynom$avg_RSME),]
optimal_param_RBF <- grid_result_RBF[which.min(grid_result_RBF$avg_RSME),]
# train the data with optimal parameters
result_linear = kernel_ridge(mX_scaled, mY, type_kernel = "linear", param = optimal_param_linear)
## linear kernel
# for the linear kernel, we get exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_linear = krr(y=mY, X= mX, kernel.type = "linear", lambda = optimal_param_linear$lambda)
result_dsmle_linear$yhat - result_linear$yhat
# for the non homogeneous polynomial kernel, exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_nonhompolynom = krr(y=mY, X= mX, kernel.type = "nonhompolynom",
lambda = optimal_param_nonhompolynom$lambda,
kernel.degree = optimal_param_nonhompolynom$degree)
result_dsmle_nonhompolynom$yhat - (result_nonhompolynom$yhat)
result_nonhompolynom = kernel_ridge(mX_scaled, mY, type_kernel = "nonhompolynom", param = optimal_param_nonhompolynom)
result_dsmle_nonhompolynom$yhat - (result_nonhompolynom$yhat)
# executes kernel ridge regression
kernel_ridge <- function(mX, mY, type_kernel = "linear", param){
# define n, p
n = nrow(mX)
p = ncol(mX)
# initial XX^T, X^TX
mXXt = mX %*% t(mX)
mXtX = t(mX) %*% mX
# create kernel
K = create_kernel(type_kernel, mXXt, mX, param)
# define J, using vector of 1s
v_1 =  matrix(1, 1, n)
J = diag(n)- ((1/n)*  t(v_1) %*% v_1)[[1]]
# define U and d
D = diag(1/eigen(K)$values)
U = eigen(K)$vectors
# find optimal intercept
optimal_mBeta_zero = (1/n) * v_1 %*% mY
# define shrunk matrix first, then find optimal intercept
mShrunk = eigen(K)$values / (eigen(K)$values + outer(rep(1,n), param$lambda))
optimal_q_hat = U %*% (mShrunk * as.vector(t(U) %*% mY))
# from q's, get other beta's
beta_from_q_hat =  inv(mXtX) %*% t(mX) %*% optimal_q_hat
# predict Y-hat
yhat = optimal_q_hat + optimal_mBeta_zero[[1]]
## adaptation to ensure correct Y-hat with RBF kernel
if (type_kernel != "linear"){
yhat = optimal_q_hat
}
# get error and RMSE
error = mY - yhat
RMSE = sqrt((1/n) * (t(error)%*%error))
# return results as list
result = list(Beta = beta_from_q_hat,
intercept = optimal_mBeta_zero[[1]],
qhat = optimal_q_hat,
yhat = yhat,
X = mX,
y = mY,
RMSE = RMSE,
K = K,
lambda = param$lambda,
param = param
)
return(result)
}
result_nonhompolynom = kernel_ridge(mX_scaled, mY, type_kernel = "nonhompolynom", param = optimal_param_nonhompolynom)
# for the non homogeneous polynomial kernel, exactly the same predicted values as the dsmle package (only a difference smaller than 15 decimals)
result_dsmle_nonhompolynom = krr(y=mY, X= mX, kernel.type = "nonhompolynom",
lambda = optimal_param_nonhompolynom$lambda,
kernel.degree = optimal_param_nonhompolynom$degree)
result_dsmle_nonhompolynom$yhat - (result_nonhompolynom$yhat)
# result for the RBF - slightly different predicted values. This is likely due to a different kernel
result_dsmle_rbf =  krr(y=mY, X= mX, kernel.type = "RBF",
lambda = optimal_param_RBF$lambda,
kernel.RBF.sigma  =  2* optimal_param_RBF$gamma)
result_dsmle_rbf$yhat - (result_RBF$yhat)
result_RBF = kernel_ridge(mX_scaled, mY, type_kernel = "RBF", param = optimal_param_RBF)
# result for the RBF - slightly different predicted values. This is likely due to a different kernel
result_dsmle_rbf =  krr(y=mY, X= mX, kernel.type = "RBF",
lambda = optimal_param_RBF$lambda,
kernel.RBF.sigma  =  2* optimal_param_RBF$gamma)
result_dsmle_rbf$yhat - (result_RBF$yhat)
# compare the kernels for RBF
## it is odd that the dsmle_rbf has the diagonal not tending towards 1...
result_RBF$K
result_dsmle_rbf$K
## compare the kernels visually
# create dataframe for plot
df_compare <- data.frame(index = 1:length(result_linear$yhat),
rbf_yhat = result_RBF$yhat,
nonhompolynom_yhat = result_nonhompolynom$yhat,
linear_yhat = result_linear$yhat,
dsmle_rbf_yhat = result_dsmle_rbf$yhat,
dsmle_nonhompolynom_yhat = result_dsmle_nonhompolynom$yhat,
dsmle_linear_yhat = result_dsmle_linear$yhat,
actual = mY)
df_compare_melted = melt(df_compare, id.vars = "index") %>%
mutate(type = ifelse(str_detect(variable, "rbf"), "RBF",
ifelse(str_detect(variable, "linear"),"linear",
ifelse(str_detect(variable, "nonhompolynom"), "non-homogeneous polynomial", "Actual"))))
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("non-homogeneous polynomial", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
titles(legend = "model")
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
title(legend = "model")
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model")
# linear of ours and
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model") +
theme_classic()
# linear of ours and
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output") +
theme_classic()
# linear of ours and
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", main = "Linear Kernel") +
theme_classic()
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", main = "Linear Kernel") +
theme_classic()
# linear of ours and
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", title = "Linear Kernel") +
theme_classic()
# linear of ours and
df_linear
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_linear = ggplot(data = df_compare_melted %>%
filter(type %in% c("linear", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", title = "Linear Kernel") +
theme_classic()
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_RBF = ggplot(data = df_compare_melted %>%
filter(type %in% c("RBF", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", title = "RBF") +
theme_classic()
# linear of ours and
df_RBF
# plots to show differences - sometimes the lines overlap exactly (linear, nonhompolynom)
df_nonhompolynom = ggplot(data = df_compare_melted %>%
filter(type %in% c("non-homogeneous polynomial", "Actual")), # in this line, one can fill in "RBF" or "linear" instead of "non-homogeneous polynomial"
aes(x = index, y = value, col = variable)) +
geom_line()+
labs(col = "Model", y = "Output", title = "non-homogeneous polynomial") +
theme_classic()
df_nonhompolynom
# Put here the packages required for subsequent analysis. P_load ensures these will be installed and loaded.
if (!require("pacman")) install.packages("pacman")
pacman::p_load(stargazer,
lmtest,
matlib)
# load data, get summary statistics
df <- read.csv("Data/DataAS1.csv")
summary(df)
### Question 4.a
# define first model, according to question 4.a.1
model_1 <- lm(birthweight ~ age + unmarried + educ ,data = df)
summary(model_1)
# define the second model, according to question 4.a.2
model_2 <- lm(birthweight ~ age + unmarried + educ + alcohol + smoker + drinks ,data = df)
summary(model_2)
### Question 4.b
# looking at the summary of model_2, the alcohol and drinks variables are statistically insignificant (e.g. cannot reject H:0)
# but the smoker variable is statistically significant.
# get table of the coefficients and their significance, showing t-values
stargazer(model_1)
stargazer(model_2)
#Answer: can reject the null hypothesis that they are all equal and zero
### Question 4.C
# test for non-linearity in model_2 - ^2 and ^3
yest <- model_2$fitted.values
yest2 <- as.matrix(yest^2)
yest3 <- as.matrix(yest^3)
# add these fitted values squared, to the power three, to the original model
test_linear <- lm(birthweight ~ age + unmarried + educ + alcohol + smoker + drinks + yest2 + yest3, data = df)
summary(test_linear)
# get residuals for the F-test
RSS_REST <- t(resid(test_linear))%*%resid(test_linear)
RSS_Orig <- t(resid(model_2)) %*% resid(model_2)
# define F-test
Ftest <- function(k, g, n, RSS_test, RSS_compare){
Ftest = ((RSS_compare - RSS_test)/g)/(RSS_test/(n-k))
print(Ftest)
Pval <- 1-pf(Ftest, g, n-k)
return(Pval)
}
nrow(df)
# calc p-val for the F-test - likely to have non-linear dynamics
pval_linear <- Ftest(k=9, g = 2, n = nrow(df), RSS_REST, RSS_Orig)
pval_linear
#Answer: p-value is 9%, e.g. a 9% chance that there are non-linear dynamics that are not captured. Cannot reject H0 at 5% significance
### Question 4.D + E
# test for non-linear dynamics now the dependent variable is logged
model_2_Log <- lm(log(birthweight) ~ age + unmarried + educ + alcohol + smoker + drinks ,data = df)
summary(model_2_Log)
stargazer(model_2_Log)
# create model with fitted values, squared and to the power three, logged
test_linear_logged <-  lm(log(birthweight) ~ age + unmarried + educ + alcohol + smoker + drinks + yest2 + yest3, data = df)
summary(test_linear_logged)
# get residuals and perform F-test
RSS_REST_Log <-  t(resid(test_linear_logged))%*%resid(test_linear_logged)
RSS_Orig_Log <- t(resid(model_2_Log)) %*% resid(model_2_Log)
pval_linear_logged <- Ftest(k=9, g=2, n=nrow(df), RSS_REST_Log, RSS_Orig_Log)
pval_linear_logged
#Answer:
# - The model gets an adjusted R-squared of 4.8%. This is worse than the model with the dependent variable unlogged (5.5%)
# - The same variables remain statistically significant
# - the F-test has a higher p-value, showing that logging the dependent variable makes it more likely to be linear
# - Question: how to interpret AIC? weird that its lower for logged model, despite having lower adjusted R-Squared
### compare linear y and log y model
summary(model_2)
summary(model_2_Log)
### Question 5: compute ML
# get dependent and independent variables
mY <- as.matrix(df$birthweight)
mX_model1 <- as.matrix(data.frame(1, df$age, df$unmarried, df$educ))
# define log-likelihood function
LL <- function(mB, mY, mX, sigma2){
n      <- nrow(mX)
e <- mY - (mX %*% mB)
logL <- .5*n*log(2*pi)-.5*n*log(sigma2)-((t(e)%*%e)/(2*sigma2))
return(-logL)
}
# set initial beta's
Beta <- c(1,1,1,1)
mBeta <- as.matrix(Beta)
# optimize log likelihood function
ML_1 <- optim(mBeta,LL,method="BFGS",hessian=T,mY=mY,mX=mX_model1, sigma2 = 1)
# has the same beta's as the lm()
ML_1$par
model_1$coefficients
