{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Which Demographic Variables help in predicting supermarket revenue? Evidence from Chicago\n",
    "\n",
    "\n",
    "Research Question: What demographic variables are important for predicting the revenue of supermarkets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data\n",
    "\n",
    "\n",
    "We work with a dataset of 45 demographic variables related to 77 supermarkets located around the Chicago area from the year of 1996. We define demographic data as data that reflects a profile of the customers; examples of such data included in our data include age, sex, income level, race, employment, homeownership, and level of education.\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<img data-src=\"CorrelationTable.PNG\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Method \n",
    "\n",
    "\n",
    "In short, our method is using an elastic net, which deals with overfitting by using a weighted average of the penalty terms applied in the LASSO and Ridge regression.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider the general form of a multivariate regression\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{\\boldsymbol{y}} = \\mathbf{X}\\boldsymbol{\\hat{\\beta}}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This has the following loss function\n",
    "\n",
    "\\begin{align*}\n",
    "    L(\\boldsymbol{\\beta}) = (\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta}).\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "But this method is prone to overfitting..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "The idea behind regularized regression methods is to apply penalty terms to multivariate regression to prevent overfitting. Elastic net is a regularized regression method that uses the following two penalty terms\n",
    "\n",
    "\\begin{align*}\n",
    "    L_1(\\boldsymbol{\\beta}) = \\lambda \\sum_{i=1}^{p} \\vert \\beta_i \\vert, \\hspace{35pt} L_2(\\boldsymbol{\\beta}) = \\lambda \\boldsymbol{\\beta}^T\\boldsymbol{\\beta}.\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Zhou and Hastie (2005) show that this method works better than using the LASSO method (which just uses $L_1$) when the independent variables are highly correlated - which makes it a good method for our use-case.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the elastic net, we minimize the following majorization function. \n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    L(\\boldsymbol{\\beta}) = (\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta})^T(\\boldsymbol{y} - \\mathbf{X}\\boldsymbol{\\beta}) + \\lambda \\alpha \\sum_{i=1}^{p} \\vert \\beta_i \\vert + \\lambda (1-\\alpha) \\boldsymbol{\\beta}^T\\boldsymbol{\\beta}.\n",
    "\\end{align*}\n",
    "In order to find the $\\boldsymbol{\\beta}$ at which $L(\\boldsymbol{\\beta})$ is minimized, we use the majorize-minimization algorithm (MM). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We use the following majorization function: \n",
    "\\begin{align*}\n",
    "    L(\\boldsymbol{\\beta})=\\frac{1}{2}\\boldsymbol{\\beta}^T(\\mathbf{A})\\boldsymbol{\\beta} - n^{-1}\\boldsymbol{\\beta}^T\\mathbf{X}^T\\boldsymbol{y} + c ,\\\\\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mathbf{A} = n^{-1}\\mathbf{X}^T\\mathbf{X} + \\lambda(1+\\alpha)I + \\lambda \\alpha \\mathbf{D}, \\hspace{35pt}\n",
    "    \\mathbf{D} = \\begin{bmatrix}\n",
    "    \\frac{1}{max(\\beta_1, \\epsilon)} &0 &0 \\\\\n",
    "    0& \\ddots & 0\\\\\n",
    "    0& 0& \\frac{1}{max(\\beta_p, \\epsilon)} \\\\\n",
    "  \\end{bmatrix}, \\hspace{35pt}\n",
    "   c = \\frac{1}{2n}\\boldsymbol{y}^T\\boldsymbol{y} + \\frac{1}{2}\\lambda\\alpha  \\sum_{i=1}^{p} \\vert \\beta_i \\vert.\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We evaluate which $\\lambda$, $\\alpha$ to pick by using k-fold cross validation\n",
    "\n",
    "\n",
    "\\begin{align*}\n",
    "    RMSE = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{\\boldsymbol{y}} - \\boldsymbol{y})^2 }, \\hspace{35pt} \\bar{RMSE} = \\frac{1}{K}\\sum_{i=1}^{K} RMSE_i.\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img data-src=\"kfold.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Results\n",
    "\n",
    "We find that an $\\alpha$ of 0.2 and $\\lambda$ of 0.1 produces the best fitted model. This indicates that in our problem more emphasis should be put on the L2 penalty. This result is consistent with the findings in (Marquardt and Snee 1975), which found that in problems with highly correlated explanatory variables ridge regression performs best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img data-src=\"CoefficientTable.PNG\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img data-src=\"GLM.ComparePlot.PNG\" />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
