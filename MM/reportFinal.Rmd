---
title: "Which Variables Contribute to Air Quality? Evidence From California"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
The World Health Organization (WHO) estimates 7 million that air pollution contributes to the death of 7 million people, anually [@WHOestimation]. For policymakers, it is thus impportant to understand which factors contribute to air quality, in order to target their efforts to improve it. This study aims to pinpoint the factors that contribute to air quality, using multiple regression. The factors considered in this study are rainfall, population density, income per capita, added value of companies and adjacency to coast. To find which combination of these variables best described their relationship on air quality, we use the better subset selection algorithm [@xiong2014better]. Previous work showed this algorithm yields a better fit than a subset without optimization as the result of its monotonicity [@xiong2014better]. Our data revealed that whether or not an area is coastal was the only statistically significant variable. 

## Data{#Data}
Previous work has suggested the relation between the five chosen variables and air quality. Both natural and anthropogenic events attribute to air quality in the atmosphere. The distribution of air pollution mainly depends on the wind field [@leelHossy2014dispersion], which is quantified by the variable of adjacency to coast and rainfall in this study as they both reflect the wind field's condition.  Air quality is also influenced by the production and consumption from society, leading to emmissions [@baklanov2016megacities]. To account for this, our study includes variables on population density, income per capita and added values from companies. We use the econometrics dataset on air quality in California for 1972 [@RstudioData]. The dependent variable is an indicator of air quality (Ruoying - can you find what this indicator consists of?), the lower the better. The independent variables under study are rainfall(inch), population density(per square mile), income per capita (\$), added value of companies (\$) and adjacency to coast(binary). The dataset has 30 set of observations, each being a different metropolitan area in California. As the unit of each variable is heterogenerous, we scaled all independent variables to have a mean of 0 and a variance of 1, to ensure fair interpretation of the model coefficents. Scaling also ensures the MM algorithm converges faster. \\

Looking at the correlations between the variables, the only one that stands out is median income and value added per company with a correlation of 0.89 (see table below). This means we need to be careful with models which include both of these variables, since multicollinearity might lead to a wrong interpretation of the coefficients. 


## Method{#Method}


Where p denotes the number of independent variables used in the model, without the intercept. We use the better subset method to select the best model for an M number of independent variables, with $M \in [1,5]$. 


## Result{#Result}

The model that best describes the relationship (e.g. with the highest adjusted $R^2$) is the one with just coastal area, and value added per company. Across all the models, the only variable that is statistically significant is the coastal area variable. The table below shows the results of the standardized independent variables on the dependent variable of Y quality. The coefficient in this case tells us that one standard deviation of change in the independent variable, leads to a certain change in the dependent variable.  

\begin{table}[!htbp] \centering 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{5}{c}{\textit{Dependent variable:}} \\ 
\cline{2-6} 
\\[-1.8ex] & \multicolumn{5}{c}{Air Quality} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5)\\ 
\hline \\[-1.8ex] 
 Coastal Area (1 = coastal, 0 = non-coastal) & $-$13.73$^{***}$ & $-$13.83$^{***}$ & $-$16$^{***}$ & $-$14.67$^{***}$ & $-$15.54$^{***}$ \\ 
  & (-2.54) & ($-$2.93) & ($-$3.37) & ($-$3.00) & ($-$3.19) \\ 
  & & & & & \\ 
 Value Added & - & 9.26 & - & 10.19 & 4.24 \\ 
  &  & (0.92) &  & (0.98) & (0.41) \\ 
  & & & & & \\ 
 Median Income & - & - & 10.0 & - & 6.9 \\ 
  &  &  & (0.969) &  & (0.64) \\ 
  & & & & & \\ 
 Population Density & - & - & - & $-$2.66 & $-$3.04 \\ 
  &  &  &  & ($-$0.58) & ($-$0.66) \\ 
  & & & & & \\ 
  Rain  & - & - & - & - & 3.38 \\ 
  &  &  &  & & 0.73 \\ 
  & & & & & \\ 
 Intercept & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ \\ 
  & (21.34) & (23.075) & (23.24) & (23.43) & (23.69) \\ 
  & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 30 & 30 & 30 & 30 & 30 \\ 
R$^{2}$ & 0.240 & 0.349 & 0.359 & 0.369 & 0.383 \\ 
Adjusted R$^{2}$ & 0.21 & 0.30 & 0.29 & 0.27 & 0.25 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 

To make the result on coastal areas a bit more interpretable, If we rescale the coefficient of the coastal area (by adding the mean, and dividing by the standard deviation), we can infer that being an coastal area leads to change of-28.17 points in the air quality index, which means that coastal areas have higher air quality.  

## Limitations and Future Research

Apart from the limitations to our dataset, we should note several other limitations to our applied method. First, there are other variables that affect air polution that we did not consider. An example is ..... (add citation). Second, while our method is appropriate for estimating linear relationships, previous research has shown that the effects of certain variables on air pollution is non-linear - one such example is environmental regulation (... add citation). \\
\\
Future research should aim to add more areas, years, and variables to our analysis to consider other possibilities. In addition, while there is some existing research on air quality in coastal areas (see....), it is worth further disentangling the geographic aspects of coastal areas that lead to an improvement in air quality, to see if those aspects that can be recreated in non-coastal areas. 


## Conclusion
(add sentence)
Our results suggest that coastal areas experience much better air quality than non-coastal areas. This has implications for policymakers. It provides evidence for initiatives that move residents to coastal areas to expose them to enhanced air quality. One such initiative was recently announced in Taiwan(.....). On the other hand, our results suggest that policies which aim to limit population density or economic activity might be less effective than previously thought, since we found no statistically significant relationship between these variables and air pollution. \\




## Reference{#Reference}




### Functions 
```{r}
# calcRSS: Calculates the residual squared errors for a multiple regression of the form Y = XBeta + e
# 
# Parameters: 
#   mX: Matrix of n x p (n = observations, p = independent variables)
#   mY: Column matrix of n x 1 dependent variables (n = observations)
#   mBeta: Column Matrix of p x 1 coefficients
#   
# Output:
#   ESquared: double, residual squared errors

calcRSS <- function(mX, mY,mBeta){
  
  # calculate the errors
  mE <-  mY - mX %*% mBeta
  
  # get errors squared
  ESquared <- t(mE) %*% mE
  
  # return the residual sum of squared errors
  return(ESquared[1,1])
  
}

# calcCovar: Calculates the covariance matrix 
#
# Parameters: 
#   RSS: Residual squared errors
#   mXtX: pxp matrix, created from independent variables (X), multiplied with itself
#   n: double, number of observations
#   p: double, number of variables
#
# Output:
#   Covar: matrix, covariance matrix

calcCovar <- function(RSS, mXtX,n, p){
  
  # est. for sigma squared
  SigmaSquared <- (RSS) / (n - p -1)
  
  Covar <- SigmaSquared * as.matrix(inv(mXtX))
  
  return(Covar)
  
}

# calcSignificance: Calculates the statistical significance of a set of beta's
#
# Parameters: 
#   RSS: Residual squared errors
#   mXtX: pxp matrix, created from independent variables (X), multiplied with itself
#   n: double, number of observations
#   p: double, number of variables
#   mBetaEst: matrix of estimated Beta's
#
# Output:
#   dfSignificance: dataframe, containing the results on statistical signficance

calcSignificance <- function(RSS, mXtX, n,p, mBetaEst){
  
  # get covariance matrix
  mCovar <- calcCovar(RSS,mXtX,n,p)
  
  # calculate the standard deviations
  stdev <- sqrt(diag(mCovar))
  
  # define t, which is t-distributed with n-p-1 degrees of freedom 
  t <- mBetaEst/stdev
  pval <- 2*pt(-abs(t),df=n-p-1)
  
  dfSignificance <- data.frame(BetaEst = mBetaEst, 
                               stdev = stdev, 
                               t = t, 
                               pval = pval)
  
  return(dfSignificance)
}

# calcLargestEigen: Calculates the largest eigenvalue of an matrix of independent variables
# 
# Parameters: 
#   mX: Dataframe of n x p (n = observations, p = independent variables)
#   
# Output:
#   LargestEigenval: float, largest eigenvalue of said matrix

calcLargestEigen <- function(mX){
  
  # get the eigenvalues of X 
  EigenValX <- eigen(mX)$values
  
  # from these eigenvalues, get the largest one
  LargestEigenVal <- max(EigenValX, na.rm = TRUE)
  
  return(LargestEigenVal)
  
}

# CalcStepScore: Calculates the % improvement between the k-1th and kth set of beta's
# 
# Parameters:
#   prevBeta: double, k-1th beta
#   currbeta: double, kth beta
#   mX: Dataframe of n x p (n = observations, p = independent variables)
# 
# Output: 
#   StepScore; double, % improvement between the RSS of the two sets of beta's

calcStepScore <- function(mX,mY, prevBeta, currBeta){
  
  # difference in RSS between previous and current set of beta's
  diffRSS <- (calcRSS(mX,mY,prevBeta) - calcRSS(mX,mY,currBeta))
  
  # divide difference with previous score to get % change
  StepScore <- diffRSS /calcRSS(mX,mY,prevBeta)
  
  return(StepScore)
  
}

# calcRsquared
#
# Calculates the r-squared
#
# Parameters:
#   Y: matrix, the true dependent variable   
#   Yest: matrix, the predicted dependent variable
#   (optional) adjusted: if True, return adjusted r squared
#   (optional) p: if adjusted is calculated, add number of variables
# 
# Output:
#   Rsquared: double, the Rsquared or adjusted Rsquared for a linear model

calcRsquared <- function(mY, mYest, adjusted = FALSE, p=0, n=0){
  
  # standardize Y, and Yest (mean of 0)
  mStandY = mY - mean(mY)
  mStandYest = mYest - mean(mYest)
  
  # calculate Rsquared
  numerator <- (t(mStandY) %*% mStandYest)^2
  denominator <- (t(mStandY) %*% mY) %*% (t(mStandYest) %*% mStandYest)
  resultRsquared <- (numerator/denominator)
  
  # if want adjusted R squared, 
  if(adjusted){
    
    adjRsquared = 1 - (((1-resultRsquared)*(n - 1))/(n-p-1))
    resultRsquared <- adjRsquared
  }
  
  return(resultRsquared)
  
}

# calcModelMM
#
# Calculates a linear model, using the majorization in minimization (MM) algorithm
#
# Parameters:
#   X: Dataframe of n x p (n = observations, p = independent variables)
#   Y: Dataframe of n x 1 dependent variables (n = observations)
#   e: epsilon, parameter for threshold of improvement after which the algorithm should halt
#   nBeta: number of variables one wants to use
#
# Output:
#   result: dataframe with attributes of the model: 
#       - Beta: dataframe, the calculated Beta's
#       - RSS: double, Sum of squared residuals
#       - Yest: dataframe, the predicted Y
#       - Rsquared: double, R^2 for the predicted Y
#       - AdjRsquared: Adjusted Rsquared
#       - Significance results: dataframe with significance results on the beta's
#       - Residuals: dataframe, Y - Yest.
#

calcModelMM <- function(mX,mY,e, nBeta){
  
  # get number of observations, and number of variables minues the intercept
  n <- nrow(mX)
  p <- ncol(mX) - 1
  
  # check the user has filled in an appropriate amount of beta's
  if(nBeta > p + 1){
    stop("You want to use more variables than there are in the dataset of independent variables")
  }
  
  # set the previous beta variable to initial, random beta's
  prevBeta <- runif(ncol(mX), min=0, max=1)
  
  # calculate X'X
  mXtX <- t(mX) %*% mX
  
  # get largest eigenvalue for the square of independent variables
  Lambda <- calcLargestEigen(mXtX)
  
  # set initial stepscore to 0, k to 1. 
  StepScore <- 0
  k <- 1
  
  # run while, either if k is equal to 1, or the improvement between k-1th and kth set of beta's is smaller than the parameter e
  while (k == 1 | StepScore > e ){
    
    # step to next k
    k <- k + 1
    
    # calculate beta's for this k
    BetaK <- prevBeta - ((1/Lambda) *  mXtX %*% prevBeta) + ((1/Lambda) * t(mX) %*% mY )
    
    # sort the beta's based on absolute value, remove the smallest ones to keep m 
    absBetaKOrdered <- order(abs(BetaK[,1]), decreasing = T)
    BetaK[!BetaK %in% BetaK[absBetaKOrdered,][1:nBeta]] <- 0
    
    # new stepscore, % difference in RSS between new Beta's and previous beta's
    StepScore <- calcStepScore(mX,mY,prevBeta,BetaK)
    
    # assign current beta's to prevBeta variable for next iteration
    prevBeta <- BetaK
    
    
  }
  
  ## Calculate several attributes of the linear model, put in dataframes or doubles

  # final Beta's
  BetaFinal <- as.matrix(BetaK)
  
  # calculate the RSS of this final est.
  RSSBetaK <- calcRSS(mX,mY, BetaK)
  
  # get the est. dependent variables
  mYest <- mX %*% BetaFinal
  
  # get the r2 and adjusted r2
  Rsquared <- calcRsquared(mY, mYest)
  adjRsquared <- calcRsquared(mY,mYest, adjusted = T,  p, n)
  
  # get the residuals
  Resi <- mY - mYest
  
  # get the results on significance
  dfSignificance <- calcSignificance(RSSBetaK, mXtX, n, p, BetaFinal)
  
  # add these attributes together as a list to make it easily accessible
  result <- list(Beta = BetaFinal, 
                  RSS = RSSBetaK, 
                  Yest = mYest,
                  Rsquared = Rsquared, 
                  adjRsquared = adjRsquared, 
                  SignificanceResults = dfSignificance,
                  Residuals = Resi, 
                  n = n,
                  p = p)
  
  
  return(result)
  
}

# findModelMM
#
# finds the best linear model, using the MM algorithm, by testing model with 1, 2...up to all variables in X
#
# Parameters:
#   mX: Matrix of n x p (n = observations, p = independent variables)
#   mY: Matrix of n x 1 dependent variables (n = observations)
#
# Output:
#   results: list with the results for each model version

findModelMM <- function(mX, mY, e){
  
  # get the number of independent variables used
  nIndVar = ncol(mX) - 1
  
  # start at m = 1, create empty list to be filled with results
  M = 1
  results <- list()
  
  # for each m, check the best model and save the results
  while(M <= nIndVar){
    
    M <- M + 1
    
    resultM <- calcModelMM(mX, mY, e, M)
    
    strSave <- paste0("Model with ", M-1, " variable(s)")
    results[[strSave]] <- resultM
    
  }
  
  return(results)
  
}

```


## Analysis 

```{r echo = T, results = 'hide'}

# load necessary packages
library(matlib)
library(stargazer)
library(sjPlot)
library(multiColl)


```


```{r echo = T, results = 'hide'}

# load the air quality data
load("Data/Airq_numeric.Rdata")

# set to dataframe
dfAirQ <- data.frame(Airq)

# select dependent variable of air quality
Yair = dfAirQ$airq

# select all other variables as independent variables
Xair = dfAirQ[,-1]

# scale the independent variables, and add an intercept to these
XairScaled <- scale(Xair)
XairIntercept <- cbind(intercept = 1, XairScaled)

# set the data to matrix format
mYair <- as.matrix(Yair)
mXairIntercept <- as.matrix(XairIntercept)

# set seed to ensure stability of results
set.seed(0)

# set e small
e <- 0.0000000001


# calculate the model with MM, for 1-5 variables. This contains all the values shown in the paper 
compareModelMM <- findModelMM(mXairIntercept, mYair, e)

```

