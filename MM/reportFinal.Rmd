---
title: "Which Variables Contribute to Air Quality? Evidence From California"
output:
  pdf_document: default
  html_document:
    df_print: paged
bibliography: bibliography.bibtex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Introduction
World Health Organization estimates 7 million that air pollution contributes to the death of 7 million people, anually [@WHOestimation]. For policymakers, it is thus impportant to understand which factors contribute to air quality, in order to target their efforts to improve it. This study aims to pinpoint the factors that contribute to air quality, using multiple regression. The factors considered in this study are rainfall, population density, income per capita, added value of companies and adjacency to coast. To find which combination of these variables best described their relationship on air quality, we use the better subset selection algorithm [@xiong2014better]. Previous work showed this algorithm yields a better fit than a subset without optimization as the result of its monotonicity [@xiong2014better]. Our data revealed that whether or not an area is coastal was the only statistically significant variable. 

## Data{#Data}
Previous work has suggested the relation between the five chosen variables and air quality. Both natural and anthropogenic events attribute to air quality in the atmosphere. The distribution of air pollution mainly depends on the wind field [@leelHossy2014dispersion], which is quantified by the variable of adjacency to coast and rainfall in this study as they both reflect the wind field's condition.  Air quality is also influenced by the production and consumption from society, leading to emmissions [@baklanov2016megacities]. To account for this, our study includes variables on population density, income per capita and added values from companies. We use the econometrics dataset on air quality in California for 1972 [@RstudioData]. The dependent variable is an indicator of air quality (Ruoying - can you find what this indicator consists of?), the lower the better. The independent variables under study are rainfall(inch), population density(per square mile), income per capita (\$), added value of companies (\$) and adjacency to coast(binary). The dataset has 30 set of observations, each being a different metropolitan area in California. As the unit of each variable is heterogenerous, we scaled all independent variables to have a mean of 0 and a variance of 1, to ensure fair interpretation of the model coefficents. Scaling also ensures the MM algorithm converges faster (Ruoying - can you add a citation for why this is the case?)

## Method{#Method}
Multiple regression is an extension of simple linear regression, which makes a linear combination of several explanatory predictors to predict the outcome of a response variable Y:

$$
\begin{align}
Y = \beta_0 + \beta_1 &X_1+\beta_2 X_2 +...+ \beta_p X_p + \epsilon,
\\
\boldsymbol{y} = \mathbf{X} \boldsymbol{\beta} + \epsilon,
\end{align}
$$

with Y denotes random response variable, X denotes random vectors of p predictor variables, $\boldsymbol{y}$ means observed n vector of response variable, $\mathbf{X}$ include n$\times$(p+1) predictor variables with first column of ones for the intercept, and $\boldsymbol{\beta}$ denotes (p+1) vector of weights$[\beta_0, \beta_1, \beta_2,...,\beta_p]^\top$.
<br/>
To minimize RSS $(\boldsymbol{\beta})$ over $\boldsymbol{\beta}$ with

$$
\begin{align}
RSS (\boldsymbol{\beta}) = \mathbf{e}^\top \mathbf{e} = (\mathbf{y-X\boldsymbol{\beta}})&^\top (\mathbf{y-X\boldsymbol{\beta})},
\\
=\mathbf{y}^\top \mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta} -2\boldsymbol{\beta}\mathbf{X}^\top \mathbf{y}.
\end{align}
$$

The difficult part of minimizing $RSS (\boldsymbol{\beta})$ lies in $\boldsymbol{\beta}^\top\mathbf{X}^\top\mathbf{X}\boldsymbol{\beta}$, thus we need to find majorizing function of the form $\lambda\boldsymbol{\beta}^\top\boldsymbol{\beta}$. Here we apply an MM-based iterative algorithm to minimize RSS($\boldsymbol{\beta}$), the basic idea is to replace original function f(x) by a simpler function, the majorizing function g(x,y). To apply an MM algorithm for multiple regression, Choose with some initial $\beta_0\in\mathbb{R}^p$ and small $\epsilon$, then compute $RSS (\boldsymbol{\beta_0})$, also compute $\lambda$ as the largest eigenvalue of $\mathbf{X}^\top\mathbf{X}$. Then set $k\leftarrow1$, while k=1 or $(RSS (\boldsymbol{\beta_{k-1}})-(\boldsymbol{\beta_{k}}))/RSS (\boldsymbol{\beta_{k-1}})>\epsilon$do $k\leftarrow k+1$, also update $\beta^{(k)} = \beta_{k-1} - \lambda^{-1}\mathbf{X}^\top\mathbf{X}\beta_{k-1} +\lambda^{-1}\mathbf{X}^\top\mathbf{y}$. As a check, print k,$RSS (\boldsymbol{\beta_{k}})$, and $RSS (\boldsymbol{\beta_{k-1}})-RSS (\boldsymbol{\beta_{k}})$.
<br/>
Besides, to find which predictors are important, we have to try to find best subset of m nonzero weights in $\beta$. For fixed m, we utilise better subset selection as a tool in our report. Better subset selection is an approximate algorithm based on MM, the same majorizing function is used as in the MM algorithm for multiple regression. We have:
<br/>
$$
\begin{align}
RSS (\boldsymbol\beta)\leq\lambda\boldsymbol\beta^\top\boldsymbol\beta&-2\lambda\boldsymbol\beta^\top\boldsymbol{u}+c_2
\\
=\lambda\sum_{j=1}^p (\beta_{j}&-u_{j})^2 + c_{3} 
\\
= g(\boldsymbol\beta,\boldsymbol\beta_{0})
\end{align}
$$
Only m values of $\beta_{j}$ can be chosen different from 0. Then sort$|u_{j}|$ from large to small and set $\beta_{j}^+$ = $u_{j}$ for the first m elements. This choice ensures that $g(\boldsymbol\beta,\boldsymbol\beta_{0})$ is minimal.




## Result{#Result}

We use the better subset method to select the best model for an M number of variables, with $M \in [1,5]$. To assess the overall fit of each model, we look at adjusted $R^2$. We use this statistic because while it summarizes the % of the variance explained by the independent variables, it accounts for the fact that adding variables could improve the $R^2$ by random chance. The model that best explains the variance in air quality is the model in which all 5 variables are included, with an adjusted-$R^2$ of 25.4%. In this model (and all the others), the only statistically significant variable is the binary variable on coastal area. In the model with the best fit, one standard deviation in the coastal area variable leads to an decrease of -15.7 points in the air quality index. Rescaling this, it estimates that if a metropolitan area is coastal, the expected decrease in the air quality index (and thus improvement of air quality) is ..... The other variables have no statistically significant effect on air quality


\begin{table}[!htbp] \centering 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{5}{c}{\textit{Dependent variable:}} \\ 
\cline{2-6} 
\\[-1.8ex] & \multicolumn{5}{c}{Air Quality} \\ 
\\[-1.8ex] & (1) & (2) & (3) & (4) & (5)\\ 
\hline \\[-1.8ex] 
 Coastal Area & $-$13.73$^{***}$ & $-$13.83$^{***}$ & $-$14.85$^{***}$ & $-$14.98$^{***}$ & $-$15.57$^{***}$ \\ 
  & (-2.54) & ($-$2.76) & ($-$2.98) & ($-$3.03) & ($-$3.19) \\ 
  & & & & & \\ 
 Value Added & - & 9.26 & 3.63 & 3.33 & 4.090 \\ 
  &  & (0.86) & (0.34) & (0.31) & (0.39) \\ 
  & & & & & \\ 
 Median Income & - & - & 6.3 & 7.2 & 6.9 \\ 
  &  &  & (0.58) & (0.67) & (0.65) \\ 
  & & & & & \\ 
 Population Density & - & - & - & $-$2.94 & $-$3.04 \\ 
  &  &  &  & ($-$0.63) & ($-$0.66) \\ 
  & & & & & \\ 
  Rain  & - & - & - & - & 3.38 \\ 
  &  &  &  & & 0.72 \\ 
  & & & & & \\ 
 Intercept & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ & 104.700$^{***}$ \\ 
  & (21.34) & (23.075) & (23.24) & (23.43) & (23.69) \\ 
  & & & & & \\ 
\hline \\[-1.8ex] 
Observations & 30 & 30 & 30 & 30 & 30 \\ 
R$^{2}$ & 0.240 & 0.349 & 0.359 & 0.369 & 0.383 \\ 
Adjusted R$^{2}$ & 0.08 & 0.214 & 0.23 & 0.24 & 0.25 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{5}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table} 


## Limitations and Future Research

Apart from the limitations to our dataset, we should note several other limitations to our applied method. First, there are other variables that affect air polution that we did not consider. An example is ..... (add citation). Second, while our method is appropriate for estimating linear relationships, previous research has shown that the effects of certain variables on air pollution is non-linear - one such example is environmental regulation (... add citation). \\
\\
Future research should aim to add more areas, years, and variables to our analysis to consider other possibilities. In addition, while there is some existing research on air quality in coastal areas (see....), it is worth further disentangling the geographic aspects of coastal areas that lead to an improvement in air quality, to see if those aspects that can be recreated in non-coastal areas. 


## Conclusion
(add sentence)
Our results suggest that coastal areas experience much better air quality than non-coastal areas. This has implications for policymakers. It provides evidence for initiatives that move residents to coastal areas to expose them to enhanced air quality. One such initiative was recently announced in Taiwan(.....). On the other hand, our results suggest that policies which aim to limit population density or economic activity might be less effective than previously thought, since we found no statistically significant relationship between these variables and air pollution. \\




## Reference{#Reference}




### Functions 
```{r}
# calcRSS: Calculates the residual squared errors for a multiple regression of the form Y = XBeta + e
# 
# Parameters: 
#   mX: Matrix of n x p (n = observations, p = independent variables)
#   mY: Column matrix of n x 1 dependent variables (n = observations)
#   mBeta: Column Matrix of p x 1 coefficients
#   
# Output:
#   ESquared: double, residual squared errors

calcRSS <- function(mX, mY,mBeta){
  
  # calculate the errors
  mE <-  mY - mX %*% mBeta
  
  # get errors squared
  ESquared <- t(mE) %*% mE
  
  # return the residual sum of squared errors
  return(ESquared[1,1])
  
}

# calcCovar: Calculates the covariance matrix 
#
# Parameters: 
#   RSS: Residual squared errors
#   mXtX: pxp matrix, created from independent variables (X), multiplied with itself
#   n: double, number of observations
#   p: double, number of variables
#
# Output:
#   Covar: matrix, covariance matrix

calcCovar <- function(RSS, mXtX,n, p){
  
  # est. for sigma squared
  SigmaSquared <- (RSS) / (n - p -1)
  
  Covar <- SigmaSquared * as.matrix(inv(mXtX))
  
  return(Covar)
  
}

# calcSignificance: Calculates the statistical significance of a set of beta's
#
# Parameters: 
#   RSS: Residual squared errors
#   mXtX: pxp matrix, created from independent variables (X), multiplied with itself
#   n: double, number of observations
#   p: double, number of variables
#   mBetaEst: matrix of estimated Beta's
#
# Output:
#   dfSignificance: dataframe, containing the results on statistical signficance

calcSignificance <- function(RSS, mXtX, n,p, mBetaEst){
  
  # get covariance matrix
  mCovar <- calcCovar(RSS,mXtX,n,p)
  
  # calculate the standard deviations
  stdev <- sqrt(diag(mCovar))
  
  # define t, which is t-distributed with n-p-1 degrees of freedom 
  t <- mBetaEst/stdev
  pval <- 2*pt(-abs(t),df=n-p-1)
  
  dfSignificance <- data.frame(BetaEst = mBetaEst, 
                               stdev = stdev, 
                               t = t, 
                               pval = pval)
  
  return(dfSignificance)
}

# calcLargestEigen: Calculates the largest eigenvalue of an matrix of independent variables
# 
# Parameters: 
#   mX: Dataframe of n x p (n = observations, p = independent variables)
#   
# Output:
#   LargestEigenval: float, largest eigenvalue of said matrix

calcLargestEigen <- function(mX){
  
  # get the eigenvalues of X 
  EigenValX <- eigen(mX)$values
  
  # from these eigenvalues, get the largest one
  LargestEigenVal <- max(EigenValX, na.rm = TRUE)
  
  return(LargestEigenVal)
  
}

# CalcStepScore: Calculates the % improvement between the k-1th and kth set of beta's
# 
# Parameters:
#   prevBeta: double, k-1th beta
#   currbeta: double, kth beta
#   mX: Dataframe of n x p (n = observations, p = independent variables)
# 
# Output: 
#   StepScore; double, % improvement between the RSS of the two sets of beta's

calcStepScore <- function(mX,mY, prevBeta, currBeta){
  
  # difference in RSS between previous and current set of beta's
  diffRSS <- (calcRSS(mX,mY,prevBeta) - calcRSS(mX,mY,currBeta))
  
  # divide difference with previous score to get % change
  StepScore <- diffRSS /calcRSS(mX,mY,prevBeta)
  
  return(StepScore)
  
}

# calcRsquared
#
# Calculates the r-squared
#
# Parameters:
#   Y: matrix, the true dependent variable   
#   Yest: matrix, the predicted dependent variable
#   (optional) adjusted: if True, return adjusted r squared
#   (optional) p: if adjusted is calculated, add number of variables
# 
# Output:
#   Rsquared: double, the Rsquared or adjusted Rsquared for a linear model

calcRsquared <- function(mY, mYest, adjusted = FALSE, p=0, n=0){
  
  # standardize Y, and Yest (mean of 0)
  mStandY = mY - mean(mY)
  mStandYest = mYest - mean(mYest)
  
  # calculate Rsquared
  numerator <- (t(mStandY) %*% mStandYest)^2
  denominator <- (t(mStandY) %*% mY) %*% (t(mStandYest) %*% mStandYest)
  resultRsquared <- (numerator/denominator)
  
  # if want adjusted R squared, 
  if(adjusted){
    
    adjRsquared = 1 - (((1-resultRsquared)*(n - 1))/(n-p-1))
    resultRsquared <- adjRsquared
  }
  
  return(resultRsquared)
  
}

# calcModelMM
#
# Calculates a linear model, using the majorization in minimization (MM) algorithm
#
# Parameters:
#   X: Dataframe of n x p (n = observations, p = independent variables)
#   Y: Dataframe of n x 1 dependent variables (n = observations)
#   e: epsilon, parameter for threshold of improvement after which the algorithm should halt
#   nBeta: number of variables one wants to use
#
# Output:
#   result: dataframe with attributes of the model: 
#       - Beta: dataframe, the calculated Beta's
#       - RSS: double, Sum of squared residuals
#       - Yest: dataframe, the predicted Y
#       - Rsquared: double, R^2 for the predicted Y
#       - AdjRsquared: Adjusted Rsquared
#       - Significance results: dataframe with significance results on the beta's
#       - Residuals: dataframe, Y - Yest.
#

calcModelMM <- function(mX,mY,e, nBeta){
  
  # get number of observations, and number of variables minues the intercept
  n <- nrow(mX)
  p <- ncol(mX) - 1
  
  # check the user has filled in an appropriate amount of beta's
  if(nBeta > p + 1){
    stop("You want to use more variables than there are in the dataset of independent variables")
  }
  
  # set the previous beta variable to initial, random beta's
  prevBeta <- runif(ncol(mX), min=0, max=1)
  
  # calculate X'X
  mXtX <- t(mX) %*% mX
  
  # get largest eigenvalue for the square of independent variables
  Lambda <- calcLargestEigen(mXtX)
  
  # set initial stepscore to 0, k to 1. 
  StepScore <- 0
  k <- 1
  
  # run while, either if k is equal to 1, or the improvement between k-1th and kth set of beta's is smaller than the parameter e
  while (k == 1 | StepScore > e ){
    
    # step to next k
    k <- k + 1
    
    # calculate beta's for this k
    BetaK <- prevBeta - ((1/Lambda) *  mXtX %*% prevBeta) + ((1/Lambda) * t(mX) %*% mY )
    
    # sort the beta's based on absolute value, remove the smallest ones to keep m 
    absBetaKOrdered <- order(abs(BetaK[,1]), decreasing = T)
    BetaK[!BetaK %in% BetaK[absBetaKOrdered,][1:nBeta]] <- 0
    
    # new stepscore, % difference in RSS between new Beta's and previous beta's
    StepScore <- calcStepScore(mX,mY,prevBeta,BetaK)
    
    # assign current beta's to prevBeta variable for next iteration
    prevBeta <- BetaK
    
    
  }
  
  ## Calculate several attributes of the linear model, put in dataframes or doubles

  # final Beta's
  BetaFinal <- as.matrix(BetaK)
  
  # calculate the RSS of this final est.
  RSSBetaK <- calcRSS(mX,mY, BetaK)
  
  # get the est. dependent variables
  mYest <- mX %*% BetaFinal
  
  # get the r2 and adjusted r2
  Rsquared <- calcRsquared(mY, mYest)
  adjRsquared <- calcRsquared(mY,mYest, adjusted = T,  p, n)
  
  # get the residuals
  Resi <- mY - mYest
  
  # get the results on significance
  dfSignificance <- calcSignificance(RSSBetaK, mXtX, n, p, BetaFinal)
  
  # add these attributes together as a list to make it easily accessible
  result <- list(Beta = BetaFinal, 
                  RSS = RSSBetaK, 
                  Yest = mYest,
                  Rsquared = Rsquared, 
                  adjRsquared = adjRsquared, 
                  SignificanceResults = dfSignificance,
                  Residuals = Resi, 
                  n = n,
                  p = p)
  
  
  return(result)
  
}

# findModelMM
#
# finds the best linear model, using the MM algorithm, by testing model with 1, 2...up to all variables in X
#
# Parameters:
#   mX: Matrix of n x p (n = observations, p = independent variables)
#   mY: Matrix of n x 1 dependent variables (n = observations)
#
# Output:
#   results: list with the results for each model version

findModelMM <- function(mX, mY, e){
  
  # get the number of independent variables used
  nIndVar = ncol(mX) - 1
  
  # start at m = 1, create empty list to be filled with results
  M = 1
  results <- list()
  
  # for each m, check the best model and save the results
  while(M <= nIndVar){
    
    M <- M + 1
    
    resultM <- calcModelMM(mX, mY, e, M)
    
    strSave <- paste0("Model with ", M-1, " variable(s)")
    results[[strSave]] <- resultM
    
  }
  
  return(results)
  
}

```


## Analysis 

```{r echo = T, results = 'hide'}

# load necessary packages
library(matlib)
library(stargazer)
library(sjPlot)
library(multiColl)


```


```{r echo = T, results = 'hide'}

# load the air quality data
load("Data/Airq_numeric.Rdata")

# set to dataframe
dfAirQ <- data.frame(Airq)

# select dependent variable of air quality
Yair = dfAirQ$airq

# select all other variables as independent variables
Xair = dfAirQ[,-1]

# scale the independent variables, and add an intercept to these
XairScaled <- scale(Xair)
XairIntercept <- cbind(intercept = 1, XairScaled)

# set the data to matrix format
mYair <- as.matrix(Yair)
mXairIntercept <- as.matrix(XairIntercept)

# set seed to ensure stability of results
set.seed(0)

# set e small
e <- 0.0000000001

# select the number of beta's you want to use in the model
nBeta <- ncol(mXairIntercept) - 1

# calculate the model using the MM algorithm, using the max (5) variables
modelMM <- calcModelMM(mXairIntercept, mYair, e, nBeta)

# calculate the model with MM, for 1-5 variables. This contains all the values shown in the paper 
compareModelMM <- findModelMM(mXairIntercept, mYair, e)

```

