# load supermarket data
library(MASS)
library(matlib)
library(caret)
library(glmnet)


diagonalD = function(beta_old, p, epsilon){
  
  # Create the diagonal vector that will create diagonal matrix D
  to_diagonalise = vector(length=p)
  
  for (i in 1:p) {
   to_diagonalise[i] = 1 / max(abs(beta_old[i]), epsilon)
  }
  # Multiply the vector containing diagonals with Identity matrix to get D
  return(to_diagonalise * diag(p))
}


elasticLoss = function(betas, A, XTy, yTy, alpha, lambda, n){
 
  tBetas = t(betas)
  # Compute constant
  constant = (1/2*n) %*% yTy + (1/2) * alpha * lambda *sum(abs(betas))
  
  return(1/2 * (tBetas%*%A%*%betas)-(1/n)*(tBetas%*%XTy) + constant)
}

calcTypeNet = function(lambda, alpha, D,p){
  lambda *(1 - alpha)*diag(p) + (lambda * alpha  * D) 
}

calcRMSE = function(X, y, est_beta, n){
  error = y - X %*% est_beta
  rsme = sqrt((1/n) * (t(error)%*%error))
}


ElasticNetEst = function(X, y, beta_init, lambda, alpha, tolerance, epsilon, max_iter = 100000){
  # Set iterations and improvement
  k = 1
  improvement = 0
  
  # Define number of regressors and datapoints
  n = nrow(X)
  p = ncol(X)
  
  # Pre-compute constants
  XTX = crossprod(X, X)

  XTy = crossprod(X,y)

  yTy = crossprod(y,y)
  
  scaledI = lambda * (1-alpha) * diag(p)

  
  D = diagonalD(beta_init, p, epsilon)
  typeNetInit = calcTypeNet(lambda, alpha, D, p)
  A = 1/n * XTX + typeNetInit
  
  
  b_prev = beta_init

  
  while (k == 1 | k < max_iter && (improvement > tolerance)) {

    # Increase number steps k
    k = k + 1
    D = diagonalD(b_prev, p, epsilon)
    typeNet = calcTypeNet(lambda, alpha, D, p)

    A = ((1/n) * XTX) + typeNet
    
    b_current = solve(A, 1/n * XTy)

    
    l_current = elasticLoss(b_current, A, XTy, yTy, alpha, lambda, n)

    l_prev = elasticLoss(b_prev, A, XTy, yTy, alpha, lambda, n)
    
    improvement = (l_prev - l_current)/l_prev
    
    b_prev = b_current

  }
  return(b_current)
}

crossValidation = function (df, k, beta_init, lambda, alpha, tolerance) {
  total_rmse = 0
  n = nrow(df)
  
  y = scale(as.matrix(df[,1]))
  X = scale(as.matrix(df[,-1]))
  
  df = scale(df)
  
  #Create 10 equally size folds
  folds = createFolds(y, k = k, list = TRUE, returnTrain = FALSE)
  #Perform 10 fold cross validation
  for(i in 1:length(folds)){
    #Split the data according to the folds
    
    test = df[folds[[i]],]
    train = df[-folds[[i]],]
    
    y_train = as.matrix(train[,1])
    X_train = as.matrix(train[,-1])
    
    y_test = as.matrix(test[,1])
    X_test = as.matrix(test[,-1])
    
    b_est = ElasticNetEst(X_train, y_train, beta_init, lambda, alpha, tolerance, epsilon)
    
    rmse = calcRMSE(X_test, y_test, as.matrix(b_est), nrow(X_test))
    total_rmse = total_rmse + rmse

  }
  
  
  avg_rmse = total_rmse / length(folds)
  
  # returns results
  result = list(alpha = alpha,
                lambda = lambda,
                avg_rmse = avg_rmse
  )
  
  return(result)

}

HyperSearch = function(df, k, grid, beta_init, tolerance){
  
  results <- data.frame(Lambda= numeric(), 
                        Alpha= numeric(),
                        avg_rmse = numeric())
  
  
  for(i in 1:nrow(grid)){
    
    lambda = as.numeric(grid[i,][1])
    alpha = as.numeric(grid[i,][2])
    
    
    result_cv = crossValidation(df, k, beta_init, lambda, alpha, tolerance)
    result_row <- c(lambda, alpha,result_cv$avg_rmse)
    results[i,] <- result_row
    
  }
  
  return(results)
  
  
}

load("supermarket1996.RData")
df = subset(supermarket1996, select = -c(STORE, CITY, ZIP, GROCCOUP_sum, SHPINDX) )  
  
y = as.matrix(df[,1])
X = as.matrix(df[,-1])

beta_init = as.matrix(runif(ncol(X), min=-5, max=5))
# lambda = 0.5102
# alpha = 0.5
tolerance = 1e-12
epsilon = 1e-12







listLambda <- 10^seq(-2, 10, length.out = 50)


listAlpha <- 0
paramGrid <- expand.grid(listLambda, listAlpha)

t <- HyperSearch(df, 10, paramGrid, beta_init, tolerance)

# plot results per lambda
plot(t$Lambda, t$avg_rmse, log = "x", col = "red", type = "p", pch = 20,
     xlab = expression(lambda), ylab = "RMSE", las = 1)



result.cv <- cv.glmnet(scale(X), scale(y), alpha = 0,
                     lambda = 10^seq(-2, 10, length.out = 50), nfolds = 10)


plot(result.cv$lambda, result.cv$cvm, log = "x", col = "red", type = "p", pch = 20,
     xlab = expression(lambda), ylab = "RMSE", las = 1)
