# Author: Floris Holstege
# Purpose: implements the elastic net
# Date: 27/10/2020

library(MASS)
library(matlib)
library(caret)
library(glmnet)



# calcTypeNet: calculates a often, used variable in subsequent computations λ(1 − α)I + λα.
# This indicates the division between the lasso (a = 1) and ridge method (a = 0)
#  
# 

calcTypeNet <- function(lambda, alpha, D,p){lambda *(1 - alpha)*diag(p) + (lambda * alpha  * D) }

#calcA: calculates the term A -which, which multiplies XtX with the TypeNet term  inv(n) * t(X)X + λ(1 − α)I + λα
#
#

calcA <- function(mXtX,n, TypeNet){ (1/n) * mXtX + TypeNet}

# calcLoss: calculates the result of the loss function for a set of Beta's
#
#

calcLoss <- function(mX,mXtX, mBeta, mY,mYtY,TypeNet, n, alpha){

  c = (1/2*n) %*% mYtY + (1/2) * alpha * sum(abs(mBeta))
  S = (t(mBeta) %*% t(mX) %*% mY)
  
  U = ((1/n) * mXtX %*% TypeNet)
  V = (1/n)* S
  
  
  Z = t(mBeta) %*% U %*% mBeta
  

  result = (1/2)* Z - V + c
  
  return(result)
  
  
}


# calcElasticNet: calculates the Elastic net results for a given lambda, alpha


calcElasticNet <- function(mX,mY,lambda,alpha,e){
  
  
  # get number of observations
  n <- nrow(mX)
  p <- ncol(mX)
  
  
  # set the previous beta variable to initial, random beta's
  prevBeta <- runif(p, min=0, max=10)
  
  # calculate X'X, y'y
  mXtX <- t(mX) %*% mX
  mYtY <- t(mY) %*% mY
  

  # set initial stepscore to 0, k to 1. 
  StepScore <- 0
  k <- 1
  
  # run while, either if k is equal to 1, or the improvement between k-1th and kth set of beta's is smaller than the parameter e
  while (k == 1 | StepScore > e ){
    
    # step to next k
    k <- k + 1
    
    
    D <- 1/max(abs(prevBeta), e) * diag(p)
    TypeNet <- calcTypeNet(lambda,alpha, D,p)
    A <- calcA(mXtX, n, TypeNet)
    

    currentBeta = (1/n) * inv(A) %*% t(mX) %*% mY
    

    prevScore <- calcLoss(mX, mXtX, prevBeta, mY, mYtY, TypeNet, n, alpha)
    newscore <- calcLoss(mX, mXtX, currentBeta, mY, mYtY, TypeNet, n, alpha)
    
    StepScore <- (prevScore - newscore)/prevScore
    
    prevBeta <- currentBeta
    
  }
  
  finalBeta <- prevBeta
  
  mYpredicted = mX %*% finalBeta
  
  error <- mY - mYpredicted
  
  RMSE <- (1/n) * (t(error) %*% error)
  
  
  result = list(alpha = alpha,
                lambda = lambda,
                RMSE = RMSE,
                Beta = finalBeta
    
  )
  
  return(result)

  
}


# kfoldEval: evaluate the hyperparameters using k-fold cross validation 


kfoldEval <- function(mX, mY,lambda, alpha, e, k){
  
  # create k folds
  folds <- createFolds(mY, k =k, list = TRUE, returnTrain = FALSE)
  totalRSME = 0
  
  
  # test the model on k-1 folds, k iterations
  for(i in seq(1, k)){
    
    mXfold <- mX[-folds[[i]],]
    mYfold <- mY[-folds[[i]],]
    
    result <- calcElasticNet(mXfold, mYfold, lambda, alpha, e)
    
    totalRSME  = totalRSME + result$RMSE


  }
  
  # average RSME of all the tests 
  AvgRSME = totalRSME / k
    
  
  # returns results
  result = list(alpha = alpha,
                lambda = lambda,
                AvgRSME = AvgRSME
  )
  
  return(result)
}


findHyperParam <- function(mX, mY, e, k, ParamCombinations){
  
  results <- data.frame(Lambda= numeric(), 
                        Alpha= numeric(),
                        AvgRSME = numeric())
  

  for(i in seq(1, length(ParamCombinations[[1]]))){
    
    print(paste0("Iteration: ", i))
    print(paste0("Lambda: ", ParamCombinations$Var1[i]))
    print(paste0("alpha: ", ParamCombinations$Var2[i]))
    
    resultKfold <- kfoldEval(mX, mY, ParamCombinations$Var1[i],ParamCombinations$Var2[i], e, k)
    
    print(paste0("Avg RMSE: ", resultKfold$AvgRSME))
    
    
    resultRow <- c(ParamCombinations$Var1[i], ParamCombinations$Var2[i],resultKfold$AvgRSME)
    results[i,] <- resultRow

  }
  
  return(results)
 
  
}

# load supermarket data
load("supermarket1996.RData")
summary(supermarket1996)

# pick dependent and indpendent variable
mY = as.matrix(supermarket1996$GROCERY_sum)
mX = as.matrix(supermarket1996[,-5:-1])

# scale the independent
mXscaled <- scale(mX)

# list of all possible lambda's, and all possible alpha's, create grid of these
listLambda <- 10^seq(-2, 10, length.out = 50)
listAlpha <- 0
ParamCombinations <- expand.grid(listLambda, listAlpha)

# find results per hyper parameter
OverviewResults <- findHyperParam(mX, mY, e=0.000000000001, k=5, ParamCombinations)
options(scipen = 999)


# plot results per lambda
plot(Overview$Lambda, Overview$AvgRSME^.5, log = "x", col = "red", type = "p", pch = 20,
     xlab = expression(lambda), ylab = "RMSE", las = 1)

# Compare with GLMnet
#
#

result.cv <- cv.glmnet(mX, mY, alpha = 0,
                       lambda = 10^seq(-2, 10, length.out = 50), nfolds = 3)
plot(result.cv$lambda, result.cv$cvm^.5, log = "x", col = "red", type = "p", pch = 20,
        xlab = expression(lambda), ylab = "RMSE", las = 1)

