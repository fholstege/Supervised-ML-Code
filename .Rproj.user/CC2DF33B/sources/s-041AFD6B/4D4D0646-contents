
setwd("C:/Users/flori/OneDrive/Documents/GitHub/Supervised ML Code/SVM")


# Packes required for subsequent analysis. P_load ensures these will be installed and loaded. 
if (!require("pacman")) install.packages("pacman")
pacman::p_load(fastDummies,
               SVMMaj,
               dplyr,
               caret, 
               mclust, 
               VeryLargeIntegers,
               matlib)

# load bank data
load("bank.RData")

# get dependent variable and transform
mY <- bank[,ncol(bank)]
mY_num <- as.matrix(ifelse(mY == "yes", 1, -1))

# get all possible independent variables
mX = bank[,-ncol(bank)]

# create dummy variables 
df_toDummy = as.matrix(data.frame(
                        mX$job,
                        mX$marital, 
                        mX$education, 
                        mX$default, 
                        mX$housing, 
                        mX$loan, 
                        mX$contact,
                        mX$month, 
                        mX$day_of_week,
                        mX$poutcome))

dummy_vars <-  dummy_columns(df_toDummy)
dummy_vars <- dummy_vars[,(1+ncol(df_toDummy)):ncol(dummy_vars)]

# select numeric variables, scale these. Leave out duration for realistic predictions
numeric_vars <- as.matrix(data.frame(
                           mX$age, 
                           mX$campaign, 
                           mX$cons.price.idx, 
                           mX$cons.conf.idx, 
                           mX$nr.employed))



# combine numeric and dummy variables
mX_var <- as.matrix(cbind(1, scale(numeric_vars),dummy_vars))

# create parameters for hinge errors
create_hinge_param <- function(mY, z, hinge = "absolute", epsilon = 1e-08, k_huber =1, mQ = NA){
  
  # when the hinge is absolute
  if(hinge == "absolute"){
  
    # add epsilon threshold to make sure its invertible
    m <- abs(1 - z)
    m <- m * (m > epsilon) + epsilon * (m <= epsilon)
  
    a <- .25 * m^-1
    b <- mY * (a + 0.25)
    
    return(list(a = a,
                b = b))

  }else if (hinge == "quadratic"){
    
    # for quadratic hinge, no need to calculate a (always identity)
    
    m <- z* (z > 1) + (1 >= z)
    b <- m * mY
    
    return(list(b = b))
    
  }else if(hinge == "huber"){
    

    
    a <- 0.5 * (k_huber + 1)^-1
    
    b <- (mY == -1) * ((mQ <= -1) * (a * mQ) + (mQ >= k_huber) * (a * mQ - 0.5) + (mQ > -1 & mQ < k_huber) * (-a)) +
      (mY == 1) * ((mQ <= -k_huber) * (0.5 + a * mQ) + (mQ >= 1) * (a * mQ) + (mQ > -k_huber & mQ < 1) * (a))
    
    c <- (mY == -1) * ((mQ <= -1) * (a * mQ^2) + (mQ >= k_huber) * (1 - (k_huber + 1)/2 + a * mQ^2) + (mQ > -1 & mQ < k_huber) * (a)) +
      (mY == 1) * ((mQ <= -k_huber) * (1 - (k_huber + 1)/2  + a * mQ^2) + (mQ >= 1) * (a * mQ^2) + (mQ > -k_huber & mQ < 1) * (a))
    
    
    
    
    

    #t_v_mY = (mY == -1) * (mQ <= -1)
    #print(t_v_mY)

    # b <- (mY == -1) * ((mQ<= -1) * (a * mQ) +
    #   (mQ >= k_huber) * (a * mQ - .5) +
    #   (mQ > -1 & mQ < k_huber) * (-a))+
    #   (mY == 1) * ((mQ <= -k_huber) * (.5 + a * mQ)+
    #   (mQ >= 1) * (a * mQ) +
    #   (mQ > -k_huber & mQ < 1) * (a))
    # 
    # #maQ <- a * mQ
    # #maQ2 <- maQ^2
    # 
    # c <- (mY == -1) * ((mQ <= -1) * (a * mQ^2) +
    #   (mQ >= k_huber) * (1 - (k_huber + 1)/2 + a * mQ^2) +
    #   (mQ > -1 & mQ < k_huber) * (a)) +
    #   (mY == 1) * ((mQ <= -k_huber) * (1 - (k_huber + 1)/2  + a * mQ^2) +
    #   (mQ >= 1) * (a * mQ^2) + (mQ > -k_huber & mQ < 1) * (a))
      
    return(list(a = a, 
                b = b, 
                c = c))
  }
  
  
}




calc_loss <- function(z, mW, lambda, hinge = "absolute", lHinge_param = NA, mQ = NA, mY = NA, k_huber){
  
  mWtW = t(mW) %*% mW
  penalty = lambda * mWtW
  
  if(hinge == "absolute"){
    
    vloss <- (1 > z) * (1 - z)
    
  }else if(hinge == "quadratic"){
    
    vloss <- (1 > z) * (1 - z)^2
    
  }else if(hinge == "huber"){
    
    if(is.na(lHinge_param) || is.na(mQ)){
      stop("No hinge paramers or prediction (Q) passed")
      
    }
    
    vloss <- (mY == 1) * (mQ <= -k_huber) * (1 - mQ -(k_huber + 1)/2) +
      (mY == 1) * (mQ > -k_huber) * (0.5 * (k_huber+1)^-1 * pmax(0,1 - mQ) ^2) +
      (mY == -1) * (mQ <= k_huber) * (0.5 * (k_huber+1)^-1 * pmax(0,1 + mQ) ^2) +
      (mY == -1) * (mQ > k_huber) * (mQ +1 -(k_huber + 1)/2)
    
  }else{
    stop("Not given an known hinge error")
  }

  return(sum(vloss) + penalty)
}



svm_mm <- function(mY, mX, hinge = "absolute", lambda = 10, epsilon = 1e-08, k_huber = 1){
  
  # set initial weights and constant. From these, create initial v = c + w
  vW = runif(ncol(mX)-1, -1, 1)
  fConstant = 0.0
  vV_current = as.matrix(c(fConstant, vW))
  
  # get n of observations in the sample, and p variables
  n = nrow(mX)
  p = ncol(mX)
  
  # define matrix p
  P = diag(p)
  P[1,1]<-0
  
  # set initial values; k, stepscore 
  k = 1
  stepscore = 0 
  
  if(hinge == "quadratic"){
    Z = inv(t(mX) %*% mX + lambda * P) %*% t(mX)
  }
  
  
  while(k ==1 || stepscore > epsilon){
    
    # get current prediction (q) and z
    mCurrent_q = mX %*% vV_current
    fCurrent_z = mY * mCurrent_q
    
    # get parameters given the y and z
    lHinge_param_current = create_hinge_param(mY = mY, 
                                              z= fCurrent_z, 
                                              hinge = hinge, 
                                              epsilon = epsilon, 
                                              k_huber = k_huber,
                                              mQ = mCurrent_q)
    lHinge_param_update = NA

    
    b = lHinge_param_current$b

    if(hinge == "quadratic"){
      
      vV_update = Z %*% b
    
    }else {
      
      # define A and b
      A = diag(as.vector(lHinge_param_current$a),n)

      # get updated v
      vV_update = solve(t(mX) %*% A %*% mX + lambda * P, t(mX) %*% b)
      
    }
    
    # get weights in previous and next 
    mW_current = tail(vV_current,-1)
    mW_update = tail(vV_update, -1)

    # get new prediction (q) and z
    mNew_q = mX %*% vV_update
    fNew_z = mY * mNew_q
    
    if(hinge == "huber"){
      
      lHinge_param_update = create_hinge_param(mY = mY, z= fNew_z, hinge = hinge, epsilon = epsilon, mQ = mNew_q, k_huber = k_huber)
    }
    
    # calculate new, and previous loss
    fCurrent_loss = calc_loss(z = fCurrent_z, mW = mW_current, lambda = lambda, hinge = hinge, lHinge_param = lHinge_param_current, mQ = mCurrent_q, k_huber = k_huber, mY = mY)
    fNew_loss = calc_loss(z = fNew_z,mW = mW_update, lambda = lambda, hinge = hinge, lHinge_param = lHinge_param_update, mQ = mNew_q,k_huber = k_huber, mY = mY)
    
    print("Old loss")
    print(fCurrent_loss)
    
    print("New loss")
    print(fNew_loss)
    
    # calculate improvement
    stepscore <- (fCurrent_loss - fNew_loss)/fCurrent_loss
    

    # check: if all predicted correctly, turns to NaN since divided by zero
    if (is.na(stepscore)){
      stepscore = 0
    }
    
    # move to next iteration
    k = k + 1
    vV_current = vV_update
    
  }

  mYhat = sign(mNew_q)
  
  resultOverview <- confusionMatrix(as.factor(mY), as.factor(mYhat))
  mConfusionTable <- resultOverview$table
  fAccuracy <- resultOverview$overall[1]

  
  lResults = list(v = vV_update,
                  mY = mY,
                  mX = mX, 
                  loss = fCurrent_loss,
                  q = mNew_q,
                  yhat = mYhat,
                  Accuracy = fAccuracy,
                  ConfusionTable = mConfusionTable)
  
  return(lResults)

  
  
}


svm_mm_cv <- function(mX, mY, lambda, folds, hinge = "absolute", k_huber = NA, epsilon = 1e-08, metric = "ARI"){
  
  fTest_metric <- 0
  
  #Perform k fold cross validation
  for(i in 1:length(folds)){
    
    #Split the data according to the folds
    vTest_id = folds[[i]]
    vTrain_id = -folds[[i]]

    # define train and test set for y and x
    mY_train <- mY[vTrain_id]
    mX_train <- mX[vTrain_id,]
    mY_test <- mY[vTest_id]
    mX_test <- mX[vTest_id,]
    

    # get result from training set
    lResult = svm_mm(mY = mY_train, mX = mX_train, hinge = hinge, lambda = lambda, epsilon = epsilon )
    mV_train <- lResult$v
    
    # calculate the predicted y for this fold
    mQ_test <- mX_test %*% mV_train
    mY_hat <- sign(mQ_test)
    
    if(metric == "ARI"){
      
      # get the confusion matrix
      fAdjRand <- adjustedRandIndex(mY_test, mY_hat)
      fTest_metric_fold <- fAdjRand
      
    }else if (metric == "misclassification"){

      # calculate misclassification
      tab <- table(mY_test, mY_hat)
      fMisclassification <- 1-sum(diag(tab))/sum(tab)
      fTest_metric_fold <- fMisclassification
      
    }
    
    # add to calculate average
    fTest_metric <- fTest_metric + fTest_metric_fold

  }
  
  # calculate average
  avg_test_metric = fTest_metric/length(folds)
  
  lResult_cv <- list(lambda = lambda,
                     metric = avg_test_metric)
  
  return(lResult_cv)
  
  
}

svm_mm_gridsearch <- function(mX, mY, lambda, hinge = "absolute", k, k_huber = NA, epsilon = 1e-08, metric = "ARI"){
  
  
  if (!is.na(k_huber)){
    
    mParamgrid = expand.grid(lambda, k_huber)
  }else {
    
    mParamgrid = expand.grid(lambda)
  }

  # create k equally size folds
  folds = createFolds(mY, k = k, list = TRUE, returnTrain = FALSE)
  
  mParamgrid$metric <- NA

  # iterate over the grid
  for(i in 1:nrow(mParamgrid)){

    # select parameters from the grid
    param <- mParamgrid[i,]
    
    # test these with k-fold
    cv_result <- svm_mm_cv(mX, mY, lambda =param[1,1], folds = folds, hinge = hinge, epsilon = epsilon, metric = metric)
    
    #save the result
    mParamgrid$metric[i] <- cv_result$metric
  }
  
  return(mParamgrid)
}
  
  





# ensure that we can reproduce the code
set.seed(123)

# pick a random sample of 1000
sample_id <- sample(4000, 1000)
sample_x = mX_var[sample_id,]
sample_y = mY_num[sample_id]


# compare our algorithm with SVM Maj - we get the same predictions (see confusion matrix), and exactly the same loss
# but still different beta's? most likely something to do with the transformation of the data...
result = svm_mm(sample_y, sample_x, lambda = 10, hinge = "huber", k_huber = 3)
result$v
result$loss
result$ConfusionTable
1 - result$Accuracy


help(svmmaj)
result_svmmaj <- svmmaj(sample_x,sample_y, lambda = 10, scale = "none",hinge = "huber")
result_svmmaj$beta 
result_svmmaj$loss
result_svmmaj$q

vLambda =10^seq(10, -5, length.out= 10)
k_huber = seq(-1,1,length.out=5)
k = 5

# cv comparison
result_svmmaj_cv <- svmmajcrossval(sample_x,sample_y, search.grid = list(lambda = vLambda) , k = k,scale = "none",hinge = "quadratic")
result_svm_mm_cv <- svm_mm_gridsearch(sample_x, sample_y, k = k, lambda = vLambda, hinge = "quadratic", metric = "ARI")


# data cleaning
colnames(result_svm_mm_cv) <- c("lambda", "ARI")

ggplot(data = result_svm_mm_cv, aes(x = lambda, y = ARI))+
  geom_line()+
  theme_minimal()


result_svmmaj_cv_linKernel <- svmmajcrossval(sample_x, sample_y, 
                                             search.grid = list(lambda = 1.0e+15),
                                             k = k, scale = "none", 
                                             hinge = "quadratic", 
                                             kernel = polydot,
                                             degree = 1)
result_svmmaj_cv_linKernel
